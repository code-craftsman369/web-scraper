import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    # å…ˆé ­ã®æ•°å­—ã¨è¨˜å·ã‚’å‰Šé™¤ï¼ˆä¾‹ï¼š1,2,ãªã©ï¼‰
    text = re.sub(r'^[\d\s]+', '', text)

    # æœ«å°¾ã®æ—¥ä»˜ã‚„æ™‚åˆ»ã‚’å‰Šé™¤ï¼ˆä¾‹ï¼š11/13(æœ¨)22:30ï¼‰
    text = re.sub(r'\d{1,2}/\d{1,2}\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)\d{1,2}:\d{2}$', '', text)
    
    # è¤‡æ•°ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    text = re.sub(r'\s+', '', text)

    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    text = text.strip()

    return text

def scrape_with_retry(url, max_retries=3):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§Webãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for attempt in range(max_retries):

        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response
        except Exception as e:
            print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼ ({attempt + 1}/{max_retries})")
            time.sleep(2)

    return None

def scrape_yahoo_news_clean():
    """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä»˜ãï¼‰"""
    url = "https://news.yahoo.co.jp/"
    
    print("=== Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ä»˜ãï¼‰ ===\n")
    
    response = scrape_with_retry(url)

    if response is None:
        print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None
    
    soup = BeautifulSoup(response.content, 'html.parser')
    links = soup.find_all('a')

    news_data = []
    seen_titles = set()

    for link in links:
        text = link.get_text(strip=True)
        href = link.get('href')

        if text and len(text) >= 20 and href:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_text = clean_text(text)

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã§ï¼‰
            if cleaned_text and len(cleaned_text) >= 15 and cleaned_text not in seen_titles:
                seen_titles.add(cleaned_text)

                news_data.append({
                    'title': cleaned_text,
                    'url': href,
                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })

                print(f"{len(news_data)}. {cleaned_text[:60]}...")
                
                if len(news_data) >= 10:
                    break

    if news_data:
        df = pd.DataFrame(news_data)
        
        # é‡è¤‡URLã‚’å‰Šé™¤
        df = df.drop_duplicates(subset=['url'], keep='first')
        
        filename = f"yahoo_news_cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"\nâœ… æˆåŠŸ: {len(df)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—ï¼ˆé‡è¤‡å‰Šé™¤å¾Œï¼‰")
        print(f"ğŸ“ ä¿å­˜: {filename}")
        
        return df
    else:
        print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

# å®Ÿè¡Œ
scrape_yahoo_news_clean()

