import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time

def scrape_with_retry(url, max_retries=3):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§Webãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 200:
                return response
            else:
                print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")

        except requests.exceptions.Timeout:
            print(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {attempt + 1}å›ç›®")
            time.sleep(2)

        except requests.exceptions.RequestException as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(2)

    return None

def scrape_yahoo_news():
    """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    url = "https://news.yahoo.co.jp/"

    print("=== Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹ ===\n")

    # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§å–å¾—
    response = scrape_with_retry(url)

    if response is None:
        print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None
    
    soup = BeautifulSoup(response.content, "html.parser")
    links = soup.find_all('a')

    news_data = []
    count = 0
    seen_titles = set()

    for link in links:
        text = link.get_text(strip=True)
        href = link.get('href')

        if text and len(text) >= 20 and href and text not in seen_titles:
            count += 1
            seen_titles.add(text)

            print(f"{count}. {text[ :50]}...")

            news_data.append({
                'title': text,
                'url': href,
                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })

            if count >= 10:
                break
    
    if news_data:
        df = pd.DataFrame(news_data)
        filename = f"yahoo_news_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')

        print(f"\nâœ… æˆåŠŸ: {len(news_data)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
        print(f"ğŸ“ ä¿å­˜: {filename}")

        return df
    else:
        print("âŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None
    
# å®Ÿè¡Œ
scrape_yahoo_news()

