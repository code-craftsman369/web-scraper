import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time

def scrape_with_retry(url, max_retries=3):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§Webãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response
        except Exception as e:
            print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼ ({attempt + 1}/{max_retries}): {str(e)[:50]}...")
            time.sleep(2)
    
    return None

def scrape_site(site_name, url):
    """æŒ‡å®šã—ãŸã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    print(f"\nğŸ“° {site_name} ã‚’å–å¾—ä¸­...")
    
    response = scrape_with_retry(url)
    
    if response is None:
        print(f"  âŒ å–å¾—å¤±æ•—")
        return []
    
    soup = BeautifulSoup(response.content, 'html.parser')
    links = soup.find_all('a')
    
    news_data = []
    count = 0
    seen_titles = set()
    
    for link in links:
        text = link.get_text(strip=True)
        href = link.get('href')
        
        if text and len(text) >= 20 and href and text not in seen_titles:
            count += 1
            seen_titles.add(text)
            
            news_data.append({
                'site': site_name,
                'title': text,
                'url': href,
                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
            
            if count >= 5:
                break
    
    print(f"  âœ… {len(news_data)}ä»¶å–å¾—")
    return news_data

def scrape_multiple_sites():
    """è¤‡æ•°ã®ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    print("=== è¤‡æ•°ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›† ===")
    
    sites = [
        ('Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'https://news.yahoo.co.jp/'),
        ('NHK NEWS WEB', 'https://www3.nhk.or.jp/news/'),
    ]
    
    all_news = []
    
    for site_name, url in sites:
        news = scrape_site(site_name, url)
        all_news.extend(news)
        time.sleep(2)
    
    if all_news:
        df = pd.DataFrame(all_news)
        filename = f"multi_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ“Š åˆè¨ˆ: {len(all_news)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
        print(f"ğŸ“ ä¿å­˜: {filename}")
        
        return df
    else:
        print("\nâŒ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

# å®Ÿè¡Œ
scrape_multiple_sites()